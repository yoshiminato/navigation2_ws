import sys
sys.path.append('/home/yoshi/navigation2_ws/src')
from parallel_controller_env.parallel_controller_env import Nav2ParallelEnv
from pettingzoo.test import parallel_api_test
from stable_baselines3.common.vec_env import VecMonitor

from future_extractor import Nav2CombinedExtractor

import rclpy
import threading
import matplotlib.pyplot as plt
import numpy as np
import math
import argparse
import time
import gymnasium as gym

from rclpy.executors import SingleThreadedExecutor

# Stable-Baselines3ã¨Supersuitï¼ˆä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ï¼‰
try:
    from stable_baselines3 import PPO, DQN, SAC
    from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv
    from stable_baselines3.common.callbacks import CheckpointCallback
    from sb3_compatible_wrapper import SB3CompatibleWrapper, wrap_env_for_sb3
    from stable_baselines3.common.evaluation import evaluate_policy
    HAS_SB3 = True
except ImportError:
    HAS_SB3 = False
    print("Warning: stable-baselines3 not installed. Run: pip install stable-baselines3")

try:
    import supersuit as ss
    HAS_SUPERSUIT = True
except ImportError:
    HAS_SUPERSUIT = False
    print("Warning: supersuit not installed. Run: pip install supersuit")

policy_kwargs = dict(
    net_arch=[256, 256],
    features_extractor_class=Nav2CombinedExtractor,
    features_extractor_kwargs=dict(features_dim=256), # ç‰¹å¾´é‡ã®å‡ºåŠ›æ¬¡å…ƒ
)

algorithm_class_map = {
    'DQN': DQN,
    'SAC': SAC,
}

algorithm_action_type_map = {
    'DQN': 'discrete',
    'SAC': 'continuous',
}


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--robot_count', type=int, default=9, help='ãƒ­ãƒœãƒƒãƒˆã®æ•°')
    parser.add_argument('--world_name',  type=str, default='square15', help='worldãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰')
    parser.add_argument('--use_rl', action='store_true', default=False, help='RLã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹')
    parser.add_argument('--rl_algorithm', type=str, default='SAC', help='RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç¨®é¡ (DQN, SAC, â€¦)')
    parser.add_argument('--train', action='store_true', help='SACå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--timesteps', type=int, default=100000, help='å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--load_model', type=str, default=None, help='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰')
    parser.add_argument('--eval', action='store_true', default=False, help='è©•ä¾¡/æ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰')
    parser.add_argument('--eval_episodes', type=int, default=100, help='è©•ä¾¡æ™‚ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°')
    args = parser.parse_args()

    if not rclpy.ok():
        rclpy.init()

    robot_count = args.robot_count
    world_name = args.world_name
    use_rl = args.use_rl
    alg_name = args.rl_algorithm
    action_type = algorithm_action_type_map.get(alg_name, 'continuous')
    alg_class = algorithm_class_map.get(alg_name, SAC)
    model_path = f"{alg_name}_nav2_model"

    print(f"ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {alg_name}, ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—: {action_type}")

    # ç’°å¢ƒã®ä½œæˆ
    env = Nav2ParallelEnv(robot_count=robot_count, world_name=world_name, use_rl=use_rl, action_type=action_type)

    # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
    if use_rl:
        if not HAS_SB3 or not HAS_SUPERSUIT:
            print("Error: stable-baselines3 and supersuit are required for training.")
            print("Install: pip install stable-baselines3 supersuit")
            rclpy.shutdown()
            sys.exit(1)
        
        print("=" * 60)
        print(f"{alg_name}å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ãƒ­ãƒœãƒƒãƒˆå°æ•°: {robot_count}")
        print(f"å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: {args.timesteps}")
        print("=" * 60)

        print("load_model:", args.load_model)
        print("eval:", args.eval)
        print("eval_episodes:", args.eval_episodes)
        print("=" * 60)
        
        # SB3äº’æ›ã«å¤‰æ›
        try:
            wrapped_env = wrap_env_for_sb3(env)
            wrapped_env = VecMonitor(wrapped_env)
        except Exception as e:
            print(f"Error: ç’°å¢ƒã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            rclpy.shutdown()
            # spin_thread.join()
            sys.exit(1)

        # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰
        if args.load_model:
            print(f"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {args.load_model}")

            match alg_name:
                case 'DQN':
                    model = alg_class.load(
                        args.load_model, 
                        env=wrapped_env,
                        exploration_initial_eps=0.1,  # 1.0ã§ã¯ãªã0.1ã‹ã‚‰å†é–‹
                        exploration_final_eps=0.05,
                    )
                case 'SAC':
                    model = alg_class.load(args.load_model, env=wrapped_env,)

        else:
            print(f"æ–°è¦{alg_name}ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")

            model = None

            match alg_name:
                case 'DQN':
                    model = alg_class(
                        "MlpPolicy",
                        wrapped_env,
                        policy_kwargs=policy_kwargs,
                        learning_rate=1e-4,
                        buffer_size=100000,
                        learning_starts=5000,
                        batch_size=256,
                        tau=0.005,
                        gamma=0.99,
                        train_freq=4,
                        target_update_interval=1000,
                        exploration_fraction=0.1,
                        exploration_initial_eps=1.0,
                        exploration_final_eps=0.05,
                        verbose=1,
                        tensorboard_log=f"./logs/{model_path}_tensorboard/"
                    )
                case 'SAC':
                    model = alg_class(
                        "MlpPolicy",
                        wrapped_env,
                        policy_kwargs=policy_kwargs,
                        learning_rate=1e-4,
                        buffer_size=50000,
                        learning_starts=2000,
                        batch_size=256,
                        tau=0.005,
                        gamma=0.99,
                        train_freq=4,
                        gradient_steps=1,
                        target_update_interval=1000,
                        verbose=1,
                        tensorboard_log=f"./logs/{model_path}_tensorboard/"
            )
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆ10000ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜ï¼‰
        checkpoint_callback = CheckpointCallback(
            save_freq=10000,
            save_path=f"./checkpoints/{model_path}/",
            name_prefix=f"{alg_name.lower()}_nav2"
        )
        # è©•ä¾¡ï¼æ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼ˆå­¦ç¿’ã‚’è¡Œã‚ãªã„ï¼‰
        if args.eval:
            print("âœ“ è©•ä¾¡/æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«å…¥ã‚Šã¾ã™...")
            mean_reward, std_reward = evaluate_policy(
                model, 
                wrapped_env, 
                n_eval_episodes=args.eval_episodes, 
                deterministic=True, # å®Œå…¨ãªæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§ãªã—ï¼‰
                render=False
            )

            print(f"ğŸ“Š çµæœ (Episodes: {args.eval_episodes})")
            print(f"å¹³å‡å ±é…¬: {mean_reward:.2f} +/- {std_reward:.2f}")

            env.close()
            rclpy.shutdown()
            sys.exit(0)

        # å­¦ç¿’é–‹å§‹
        print("å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        try:
            model.learn(
                total_timesteps=args.timesteps,
                callback=checkpoint_callback,
                log_interval=100
            )
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model.save(model_path)
            print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}.zip")
            
        except KeyboardInterrupt:
            print("\nå­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            model.save(f"{model_path}_interrupted")
            print(f"âœ“ ä¸­æ–­æ™‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}_interrupted.zip")
        except Exception as e:
            print(f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            import traceback
            traceback.print_exc()
        
        rclpy.shutdown()
        # spin_thread.join()
        sys.exit(0)
    
    else:
        print("âœ“ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§ç’°å¢ƒã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        print("=" * 60)
        print("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰")
        print("=" * 60)

        obs, infos = env.reset()
        for step_idx in range(10**6):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§å‹•ãã‚’ç¢ºä¿ï¼ˆprogress checkerã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            actions = {agent: env.action_space(agent).sample() for agent in env.agents}

            obs, rewards, terminations, truncations, infos = env.step(actions)


            # env.render()
            import cv2
            agent_visuals = []
            # ã‚¿ã‚¤ãƒ«è¡¨ç¤ºã®è¨­å®š (ä¾‹: 1è¡Œã«5å°ä¸¦ã¹ã‚‹)
            cols_count = 4 
            # å„ã‚¿ã‚¤ãƒ«ã®çµ±ä¸€ã‚µã‚¤ã‚º (Noneã®å ´åˆã¯æœ€åˆã®ç”»åƒã«åˆã‚ã›ã‚‹)
            tile_size = (200, 200) 

            COSTMAP_SIZE = 30
            BUFFER_SIZE = 4

            tile_size = (200, 200)

            for i in range (robot_count):
                agt_id = f"robot_{i+1}"
                # print(obs)
                img_size = COSTMAP_SIZE * COSTMAP_SIZE * BUFFER_SIZE

                agt_obs = None
                try:
                    agt_obs = obs[agt_id]
                except Exception as e:
                    print(f"Error: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ {agt_id} ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {e}")
                    continue
                 
                costmaps = np.array(agt_obs[:img_size].reshape((BUFFER_SIZE, COSTMAP_SIZE, COSTMAP_SIZE)))

                imgs = []
                imgs_color = []
                for b in range(BUFFER_SIZE):
                    imgs.append(np.fliplr(np.flipud(costmaps[b].T)))
                    imgs[b] = imgs[b].astype(np.uint8)
                    imgs_color.append(cv2.cvtColor(imgs[b], cv2.COLOR_GRAY2BGR))
                    imgs_color[b] = cv2.resize(imgs_color[b], tile_size)

                agent_visuals.append(imgs_color)

            # æ¨ªã«çµåˆ
            rows = []
            for i in range(len(agent_visuals)):
                row_img = cv2.hconcat(agent_visuals[i][0:BUFFER_SIZE])
                rows.append(row_img)

            # ç¸¦ã«çµåˆ
            combined_img = cv2.vconcat(rows)

            # è¡¨ç¤º
            cv2.imshow("Multi-Agent RL Monitor", combined_img)
            cv2.waitKey(1)


            # ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒterminationsã¾ãŸã¯truncationsã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ã§ã‚‚Trueãªã‚‰ãƒªã‚»ãƒƒãƒˆ
            if all(t or u for t, u in zip(terminations.values(), truncations.values())):
                env.reset()

        rclpy.shutdown()